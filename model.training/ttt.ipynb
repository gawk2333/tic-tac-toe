{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from itertools import count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 0.01\n",
    "steps_done = 0\n",
    "best_invalid = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming state_batch and action_batch are already defined\n",
    "def write_to_file(state,action,reward,expected):\n",
    "  # Create a list of tuples where each tuple represents a data point\n",
    "  data = [(torch.tensor(state,device=device), torch.tensor(action,device=device), torch.tensor(reward,device=device), torch.tensor(expected,device=device))\n",
    "           for state, action,reward,expected in zip(state,action,reward,expected)]\n",
    "\n",
    "  # Define the path to the text file where you want to store the data\n",
    "  file_path = \"data.txt\"\n",
    "\n",
    "  # Write the data to a CSV file\n",
    "  with open(file_path, 'w', newline='') as file:\n",
    "      writer = csv.writer(file)\n",
    "      # Write a header row (optional)\n",
    "      writer.writerow([\"State\", \"Action\",\"reward\",\"expected\"])\n",
    "      # Write the data\n",
    "      writer.writerows(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f52cdccf8e0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_invalid_turn_numbers(invalid_list, turn_numbers, show_result=False):\n",
    "    plt.figure(1)\n",
    "    \n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Invalids', color='tab:blue')\n",
    "    plt.plot(invalid_list, color='tab:blue')\n",
    "\n",
    "    if turn_numbers:\n",
    "        plt.twinx()\n",
    "        plt.ylabel('Turn Numbers', color='tab:red')\n",
    "        plt.plot(turn_numbers, color='tab:red')\n",
    "\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeEnv, self).__init__()\n",
    "        self.board = torch.zeros((3, 3), dtype=torch.float64)  # Initialize an empty 3x3 board\n",
    "        self.current_player = 1\n",
    "        self.observation_space = spaces.MultiBinary(9)  # 3x3 board, each cell is binary\n",
    "        self.action_space = spaces.Discrete(9)  # 9 possible moves (0-8)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = torch.zeros((3, 3), dtype=torch.float64)\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def select_action(self,state,player):\n",
    "        global steps_done\n",
    "        new_state=state\n",
    "        if player.sign == -1:\n",
    "            new_state = torch.where(state == 1, -1, torch.where(state == -1, 1, state)).clone().detach()\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float32, device=device).clone().detach()\n",
    "        \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                q_values = player.policy_net(new_state)\n",
    "                # print(\"q_value:\",q_values)\n",
    "                action = q_values.max(1)[1]  # Get the action with the highest Q-value\n",
    "                # print(\"action\",action)\n",
    "                return action\n",
    "        else:\n",
    "            return torch.tensor([self.action_space.sample()], device=device, dtype=torch.int32)\n",
    "    \n",
    "    def step(self, action, player):\n",
    "        row, col = divmod(action, 3)\n",
    "        \n",
    "        if self.board[row, col] != 0:\n",
    "            return (\n",
    "                torch.tensor(self.board.flatten(), dtype=torch.float64,requires_grad=True).clone().detach(),\n",
    "                -10,  # Negative reward for an invalid move\n",
    "                True,\n",
    "                \"Invalid move\"\n",
    "            )\n",
    "        self.board[row, col] = player.sign\n",
    "        done, reward, Info = self.check_game(player)\n",
    "        return torch.tensor(self.board.flatten(), dtype=torch.float64,requires_grad=True).clone().detach(), reward, done, Info\n",
    "    \n",
    "    def train(self, player1, player2):\n",
    "        if torch.cuda.is_available():\n",
    "            num_episodes = 500000\n",
    "        else:\n",
    "            num_episodes = 12000\n",
    "        # losses_list = []\n",
    "        # best_invalid = 100\n",
    "        invalid_list = []\n",
    "        turn_numbers = []\n",
    "        total_turn_number = 0\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Randomly choose the starting player for this episode\n",
    "            starting_player = random.choice([player1, player2])\n",
    "            players = [starting_player, player1 if starting_player == player2 else player2]\n",
    "            if i_episode % 100 == 0:\n",
    "                lowest_invalid_player = min(players, key=lambda player: player.invalid)\n",
    "                for player in players:\n",
    "                    if player.invalid > lowest_invalid_player.invalid:\n",
    "                        player.policy_net.load_state_dict(lowest_invalid_player.policy_net.state_dict())\n",
    "                if lowest_invalid_player.invalid < best_invalid:\n",
    "                    best_policy_net_state_dict = lowest_invalid_player.policy_net.state_dict()\n",
    "                    # Save the best policy network's state dictionary to a file\n",
    "                    torch.save(best_policy_net_state_dict, 'best_policy_net.pth')\n",
    "                plot_invalid_turn_numbers(invalid_list,turn_numbers)\n",
    "\n",
    "            board1 = self.reset()\n",
    "            state1 = torch.tensor(board1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            for t in count():\n",
    "                # for item in player.memory:\n",
    "                #     print(\"memoryitem:\",item)\n",
    "                player = players[t % 2]  # Alternates the players\n",
    "                action = self.select_action(state1, player)\n",
    "                observation, reward, done, Info = self.step(action.item(), player)\n",
    "                next_state = None\n",
    "                if done:\n",
    "                    if Info == \"Win\":\n",
    "                        player.win += 1\n",
    "                    if Info == \"Invalid move\":\n",
    "                        player.invalid += 1\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    # print(\"next\",next_state)\n",
    "                reward = torch.tensor([reward], device=device).clone().detach()\n",
    "                # print(\"state:\",state1)\n",
    "                # print(\"action:\",action)\n",
    "                # print(\"next_state:\",next_state)\n",
    "                # print(\"reward:\",reward)\n",
    "                player.push(state1, action, next_state, reward)\n",
    "                state1 = next_state\n",
    "                player.optimize_model()\n",
    "                target_net_state_dict = player.target_net.state_dict()\n",
    "                policy_net_state_dict = player.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "                player.target_net.load_state_dict(target_net_state_dict)\n",
    "                if done:\n",
    "                    # losses_list = [player1.losses, player2.losses]\n",
    "                    # plot_loss(losses_list)\n",
    "                    total_turn_number += t\n",
    "                    if i_episode % 100 == 0:\n",
    "                            average_turn = total_turn_number / 100\n",
    "                            turn_numbers.append(average_turn)\n",
    "                            invalid_list.append([player1.invalid, player2.invalid])\n",
    "                            plot_invalid_turn_numbers(invalid_list,turn_numbers)\n",
    "                            player1.win = 0\n",
    "                            player2.win = 0\n",
    "                            player1.invalid = 0\n",
    "                            player2.invalid = 0\n",
    "                            total_turn_number = 0\n",
    "                            # print(\"turn_number:\",average_turn)\n",
    "                    break\n",
    "\n",
    "        print('Complete')\n",
    "        # Plot the losses\n",
    "        plot_invalid_turn_numbers(invalid_list,turn_numbers, show_result=True)\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    class TicTacToeEnv(gym.Env):\n",
    "        metadata = {'render.modes': ['human']}\n",
    "        \n",
    "        symbols = ['O', ' ', 'X'];\n",
    "\n",
    "        def __init__(self):\n",
    "            self.action_space = spaces.Discrete(9)\n",
    "            self.observation_space = spaces.Discrete(9*3*2) # flattened\n",
    "        def step(self, action):\n",
    "            done = False\n",
    "            reward = 0\n",
    "\n",
    "            p, square = action\n",
    "            \n",
    "            # check move legality\n",
    "            board = self.state['board']\n",
    "            proposed = board[square]\n",
    "            om = self.state['on_move']\n",
    "            if (proposed != 0):  # wrong player, not empty\n",
    "                print(\"illegal move \", action, \". (square occupied): \", square)\n",
    "                done = True\n",
    "                reward = -1 * om  # player who did NOT make the illegal move\n",
    "            if (p != om):  # wrong player, not empty\n",
    "                print(\"illegal move  \", action, \" not on move: \", p)\n",
    "                done = True\n",
    "                reward = -1 * om  # player who did NOT make the illegal move\n",
    "            else:\n",
    "                board[square] = p\n",
    "                self.state['on_move'] = -p\n",
    "\n",
    "            # check game over\n",
    "            for i in range(3):\n",
    "                # horizontals and verticals\n",
    "                if ((board[i * 3] == p and board[i * 3 + 1] == p and board[i * 3 + 2] == p)\n",
    "                    or (board[i + 0] == p and board[i + 3] == p and board[i + 6] == p)):\n",
    "                    reward = p\n",
    "                    done = True\n",
    "                    break\n",
    "            # diagonals\n",
    "            if((board[0] == p and board[4] == p and board[8] == p)\n",
    "                or (board[2] == p and board[4] == p and board[6] == p)):\n",
    "                    reward = p\n",
    "                    done = True\n",
    "                    \n",
    "            return self.state, reward, done, {} \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = {}\n",
    "        self.state['board'] = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self.state['on_move'] = 1\n",
    "        return self.state\n",
    "    def render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "        print(\"on move: \" , self.symbols[self.state['on_move']+1])\n",
    "        for i in range (9):\n",
    "            print (self.symbols[self.state['board'][i]+1], end=\" \");\n",
    "            if ((i % 3) == 2):\n",
    "                print();\n",
    "    def move_generator(self):\n",
    "        moves = []\n",
    "        for i in range (9):\n",
    "            if (self.state['board'][i] == 0):\n",
    "                p = self.state['on_move']\n",
    "                m = [p, i]\n",
    "                moves.append(m)\n",
    "        return moves\n",
    "    # def render(self, mode='human'):\n",
    "    #     # Visualize the Tic-Tac-Toe board (optional)\n",
    "    #     pass\n",
    "\n",
    "    def check_game(self,player):\n",
    "        board = self.board\n",
    "        new_board = board\n",
    "        if player.sign == -1:\n",
    "            new_board = torch.where(board == 1, -1, torch.where(board == -1, 1, board))\n",
    "        for i in range(3):\n",
    "            if all(new_board[i, :] == 1) or all(new_board[:, i] == 1):\n",
    "                return True, 1 , \"Win\"\n",
    "        if torch.all(torch.diag(new_board) == 1) or torch.all(torch.diag(torch.fliplr(new_board)) == 1):\n",
    "            return True, 1, \"Win\"\n",
    "        if torch.all(board != 0):\n",
    "            print(\"draw\")\n",
    "            return True, 0 ,\"Draw\"\n",
    "        return False, 0 , \"Game ongoing\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "      super(DQN, self).__init__()\n",
    "      self.layer1 = nn.Linear(n_observations, 128)\n",
    "      self.layer2 = nn.Linear(128, 256)\n",
    "      self.layer3 = nn.Linear(256, 512)\n",
    "      self.layer4 = nn.Linear(512, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = F.relu(self.layer1(x))\n",
    "      x = F.relu(self.layer2(x))\n",
    "      x = F.relu(self.layer3(x))\n",
    "      return self.layer4(x)\n",
    "\n",
    "best_DQN = DQN(9, 9).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    def __init__(self, capacity,sign,opponent=None):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.losses = []\n",
    "        self.sign = sign\n",
    "        self.policy_net = best_DQN\n",
    "        self.target_net = best_DQN\n",
    "        self.win = 0\n",
    "        self.invalid = 0\n",
    "        self.opponent = opponent\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "    def print_memory_size(self):\n",
    "        print(\"player:\",self.sign,\"memory_size:\",len(self.memory))\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        # Randomly choose some samples from memory\n",
    "        transitions = self.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool).clone().detach()\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "        \n",
    "        # Classify the data of each type\n",
    "        state_batch = torch.cat([s.requires_grad_(True) for s in batch.state]).to(device)\n",
    "        action_batch = torch.cat([a.requires_grad_(True) for a in batch.action]).to(device)\n",
    "        reward_batch = torch.cat([r.requires_grad_(True) for r in batch.reward]).to(device)\n",
    "        state_batch = state_batch.view(-1, 9)\n",
    "\n",
    "\n",
    "        # Convert indices to one-hot encoded vectors\n",
    "        one_hot = torch.zeros((len(action_batch), 9), device=device)\n",
    "        one_hot = one_hot.scatter(1, action_batch.unsqueeze(1).long(), 1)\n",
    "        one_hot.requires_grad = True\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, one_hot.to(torch.int64)).requires_grad_(True)\n",
    "\n",
    "        non_final_next_states = non_final_next_states.view(-1, 9)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        # print(\"state:\",state_batch)\n",
    "        # print(\"action:\",action_batch)\n",
    "        # print(\"reward:\",reward_batch)\n",
    "        # print(\"expected:\",expected_state_action_values.unsqueeze(1))\n",
    "        # write_to_file(state_batch,action_batch,reward_batch,expected_state_action_values)\n",
    "        \n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        # self.losses.append(loss.item())\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        # Convert values to float32 tensors\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "        if next_state != None:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "        if len(self.memory) >= self.memory.maxlen:\n",
    "            self.memory.popleft()  # Remove the oldest transition\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m player1\u001b[39m.\u001b[39mopponent \u001b[39m=\u001b[39m player2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m game \u001b[39m=\u001b[39m TicTacToeEnv()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m game\u001b[39m.\u001b[39;49mtrain(player1,player2)\n",
      "\u001b[1;32m/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m player\u001b[39m.\u001b[39mpush(state1, action, next_state, reward)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m state1 \u001b[39m=\u001b[39m next_state\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m player\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m target_net_state_dict \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39mtarget_net\u001b[39m.\u001b[39mstate_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m policy_net_state_dict \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mstate_dict()\n",
      "\u001b[1;32m/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(state_action_values, expected_state_action_values\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/tic-tac-toe/tic-tac-toe/model.training/ttt.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# self.losses.append(loss.item())\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env = TicTacToeEnv()\n",
    "env2 = TicTacToeEnv()\n",
    "\n",
    "player1 = Player(10000,1)\n",
    "player2 = Player(10000,-1,player1)\n",
    "player1.opponent = player2\n",
    "\n",
    "game = TicTacToeEnv()\n",
    "game.train(player1,player2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_loss(losses_list, show_result=False):\n",
    "#     plt.figure(1)\n",
    "#     if show_result:\n",
    "#         plt.title('Result')\n",
    "#     else:\n",
    "#         plt.clf()\n",
    "#         plt.title('Training...')\n",
    "    \n",
    "#     for i, losses in enumerate(losses_list):\n",
    "#         loss_num = torch.tensor(losses, dtype=torch.float)\n",
    "#         x_data = range(len(loss_num))\n",
    "#         color = 'C{}'.format(i)  # Use different color for each loss\n",
    "#         plt.plot(x_data, loss_num, label=f'Loss {i + 1}', color=color)\n",
    "        \n",
    "#         if len(loss_num) >= 100:\n",
    "#             means = loss_num.unfold(0, 100, 1).mean(1).view(-1)\n",
    "#             means = torch.cat((torch.zeros(99), means))\n",
    "#             plt.plot(x_data, means.numpy(), linestyle='--', color=color, alpha=0.7)\n",
    "    \n",
    "#     plt.xlabel('Training Step')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "#     plt.pause(0.001)\n",
    "    \n",
    "#     if is_ipython:\n",
    "#         if not show_result:\n",
    "#             display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "#         else:\n",
    "#             display.display(plt.gcf())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(next_state: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([4], device='cuda:0', dtype=torch.int32)\n",
      "(reward: 0\n",
      "(done: False\n",
      "(next_state: tensor([ 0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "(reward: 0\n",
      "(done: False\n",
      "q_value: tensor([[ 0.0150,  0.0890,  0.0360, -0.0364,  0.1386, -0.0251,  0.0621,  0.0663,\n",
      "         -0.1599]], device='cuda:0')\n",
      "action tensor([4], device='cuda:0')\n",
      "(next_state: tensor([ 0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([4], device='cuda:0')\n",
      "(reward: -10\n",
      "(done: True\n",
      "(next_state: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([5], device='cuda:0', dtype=torch.int32)\n",
      "(reward: 0\n",
      "(done: False\n",
      "q_value: tensor([[ 0.0234,  0.1425,  0.0855, -0.0592,  0.1226, -0.0882,  0.1023,  0.0219,\n",
      "         -0.1762]], device='cuda:0')\n",
      "action tensor([1], device='cuda:0')\n",
      "(next_state: tensor([ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([1], device='cuda:0')\n",
      "(reward: 0\n",
      "(done: False\n",
      "(next_state: tensor([ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.], dtype=torch.float64)\n",
      "(action: tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "(reward: -10\n",
      "(done: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23705/1122861954.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_state = torch.tensor(new_state, dtype=torch.float32, device=device).unsqueeze(0).clone().detach()\n",
      "/tmp/ipykernel_23705/969265191.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.board.flatten(), dtype=torch.float64,requires_grad=True).clone().detach(), reward, done, Info\n",
      "/tmp/ipykernel_23705/969265191.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.board.flatten(), dtype=torch.float64,requires_grad=True).clone().detach(),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_policy_net_state_dict = torch.load('best_policy_net.pth')\n",
    "best_DQN.load_state_dict(best_policy_net_state_dict)\n",
    "player3 = Player(10000,1)\n",
    "player4 = Player(10000,-1,player3)\n",
    "player3.opponent = player4\n",
    "\n",
    "starting_player = random.choice([player3, player4])\n",
    "players = [starting_player, player3 if starting_player == player4 else player4]\n",
    "\n",
    "for i_episode in range(2):\n",
    "    board = env.reset()\n",
    "    test_state = board\n",
    "    for t in count():\n",
    "        player = players[t % 2]\n",
    "        # for item in player.memory:\n",
    "        #     print(\"memoryitem:\",item)\n",
    "        action = select_action(test_state,player)\n",
    "        next_state, reward, done, Info = env.step(action.item(),player)\n",
    "        print(\"(next_state:\",next_state)\n",
    "        print(\"(action:\",action)\n",
    "        print(\"(reward:\",reward)\n",
    "        print(\"(done:\",done)\n",
    "        if done:\n",
    "            # plot_loss(losses_list)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
